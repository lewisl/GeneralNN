


##################################################################################################
# Defining the hidden layers and output layer of the model
##################################################################################################

[layer.1]
    linear          = true    # or "affine", "simple"  this is the default so can be left out
    activation      = "relu"
    units           = 80

[layer.2]
    linear          = true    # or "affine", "simple"  this is the default so can be left out
    activation      = "relu"
    units           = 80

# [layer.3]
#     activation      = "relu"
#     units           = 80

[layer.output] # should this be an element in the layer array?--we can always stick it there
# required
    classify        = "softmax"
    # labels          = 10  # this shouldn't be needed as the targets in training data provide this
                          # how far should we go to allow defining a model without any valid data?

##################################################################################################
# Defining the data
##################################################################################################

# [input] # should this be an element in the layer array?--we can always stick it there  NO: use layer to mean hidden layer
# # required
#     norm_mode       = "none"   # put this in training?  so far, it's the only thing except for image data
#     # don't define dimensions here--provided by the input training data matrix
#     # don't define whether test data is provided or not--use data inputs API for that


##################################################################################################
# Controlling the training process: all of these settings can be placed under the heading "training"
##################################################################################################

[training]                  # required
    epochs              = 10  # required
    alpha               = 0.74  # learning rate or nu in some notation; required
    learn_decay         = [0.5, 4.0] # or {factor = 0.5, steps = 4.0}
    quiet               = true
    initializer         = "xavier"
    scale_init          = 2.2
    bias_initializer    = 0.3
    norm_mode           = "none"
    # [minibatch]
    do_batch        = true
    do_batch_norm   = true  # this can be by layer, but minibatch applies to the entire training run
    mb_size_in      = 50
    norm_after      = "linear"    # or "activation"--not implemented, after linear is default
    # [optimization]
    opt             = "adam"
    opt_params      = [0.9, 0.999]
    # [regularization]
    reg             = "L2"  # "Maxnorm", "L2" or "L1"
    maxnorm_lim     = 4.0   # ignored if reg is not "Maxnorm"
    lambda          = 0.000191   # only applies to L2 or L1--ignored otherwise


# [minibatch]                             # global--applies to all layers; if excluded no minibatches
#     do_batch        = true
#     do_batch_norm   = true  # this can be by layer, but minibatch applies to the entire training run
#     mb_size_in      = 50
#     norm_after      = "linear"    # or "activation"--not implemented, after linear is default


# [optimization]                  # optional: if absent, no optimzation
#     opt             = "adam"
#     opt_params      = [0.9, 0.999]

# [regularization]                # global--applies to all hidden layers--this could be done per layer--not implemented
# # optional--if absent, no regularization
#     reg             = "L2"  # "Maxnorm", "L2" or "L1"
#     maxnorm_lim     = 4.0   # ignored if reg is not "Maxnorm"
#     lambda          = 0.000191   # only applies to L2 or L1--ignored otherwise




##################################################################################################
# output from the training run
##################################################################################################

[results]
    # all = true  # this is the default--and what you get if you don't include the results table
    train_inputs = true
    train_targets = true
    train_preds = true
    test_inputs = true
    test_targets = true
    test_preds = true
    hyper_params = true
    wgts = true
    batchnorm_params = true

[plotting]          # optional: if absent, no plotting or plot data
    stats           =  ["Train", "Learning", "Test", "epoch"]       # ["Train", "Learning", "Test", "Cost", "epoch"]    # or "batch"
    plot_now        = true